{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9],\n",
    "              [0,0,1]])\n",
    "\n",
    "np.argmax(A, axis = 1) # 取行最大 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Value Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部署算法\n",
    "# V0 → V* → π*  贝尔曼 最优算子\n",
    "def value_iteration(env, tol=1e-10):\n",
    "\n",
    "    # 初始化\n",
    "    v = np.zeros(env.n_states)                    # initialize value function\n",
    "    q = np.zeros((env.n_states, env.n_actions))   # initialize Q-value function\n",
    "    \n",
    "    # 值迭代\n",
    "    while True:\n",
    "        v_old = np.copy(v)    \n",
    "\n",
    "        ######## 贝尔曼-最优算子 ########\n",
    "\n",
    "        # 更新 Q\n",
    "        for a in range(env.n_actions):\n",
    "            q[:, a] = env.r[:,a] + env.gamma * env.sparseT[a].dot(v)                          #00ff00\n",
    "            # q = 9x4 矩阵 (代表每个(s,a)组合下的 回报期望 = 即刻回报 + 折扣未来回报)\n",
    "\n",
    "        # 更新 V\n",
    "        v = np.max(q,axis = 1)                                                                #00ff00\n",
    "        # v = 9x1 向量 (选出 q 每行最大值 = 最优的 action 使得回报最大)\n",
    "\n",
    "        # 终止条件\n",
    "        if np.linalg.norm(v - v_old) < tol: \n",
    "            v_star  = v\n",
    "            pi_star = np.argmax(q,axis=1)\n",
    "            break\n",
    "            \n",
    "    return v_star, pi_star\n",
    "\n",
    "\n",
    "# 检验算法\n",
    "# π → V^π      贝尔曼 策略算子\n",
    "def evaluate_policy_sequence(pi, env, tol=1e-10):\n",
    "\n",
    "    v = np.zeros(env.n_states)                   # 初始化 V(s)\n",
    "    q = np.zeros((env.n_states, env.n_actions))  # 初始化 Q(s,a)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        v_old = np.copy(v) \n",
    "\n",
    "        ######## 贝尔曼-策略算子 ########\n",
    "\n",
    "        # 更新 Q\n",
    "        for a in range(env.n_actions):\n",
    "            q[:, a] = env.r[:,a] + env.gamma * env.sparseT[a].dot(v)      # calculate Q-value #0000ff 与上面一样\n",
    "            # q = 9x4 矩阵 (代表每个(s,a)组合下的 回报期望 = 即刻回报 + 折扣未来回报)\n",
    "\n",
    "        # 更新 V\n",
    "        for s in range(env.n_states):\n",
    "            v[s] = q[s,pi[s]]      # calculate value function by v(s) = Q(s,pi(s))            #0000ff 与上面不一样，上面使用 max 这里使用 π(s)\n",
    "\n",
    "\n",
    "        # 终止条件\n",
    "        if np.linalg.norm(v - v_old) < tol: # convergence criterion\n",
    "            v_pi = v\n",
    "            break\n",
    "                \n",
    "    return v_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Policy Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步 策略算子\n",
    "# π_(t) → V^π, Q^π\n",
    "def evaluate_policy(pi, env, tol=1e-10):\n",
    "\n",
    "    v = np.zeros(env.n_states)\n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "\n",
    "    while True:\n",
    "\n",
    "        v_old = np.copy(v)\n",
    "\n",
    "        ######## 贝尔曼-策略算子 ########\n",
    "\n",
    "        # 更新 Q\n",
    "        for a in range(env.n_actions):\n",
    "            q[:, a] = env.r[:, a] + env.gamma * env.sparseT[a].dot(v)                       #0000ff 与上面一样\n",
    "\n",
    "        # 更新 V\n",
    "        for s in range(env.n_states):\n",
    "            #v[s] = q[s, :].dot(pi[s])  #TODO #0000ff 随机策略 = 概率分布\n",
    "            v[s] = q[s, pi[s]]          #TODO #0000ff 固定策略 = 确定性\n",
    "\n",
    "\n",
    "        # 终止条件\n",
    "        if np.linalg.norm(v - v_old) < tol:\n",
    "            v_pi = v\n",
    "            q_pi = q\n",
    "            break\n",
    "\n",
    "    return v_pi, q_pi\n",
    "\n",
    "\n",
    "# 第二步 策略优化\n",
    "# Q^π → π_(t+1)\n",
    "def get_greedy_policy(q_pi):\n",
    "    policy = np.argmax(q_pi, axis = 1)\n",
    "    return policy\n",
    "\n",
    "\n",
    "# 算法部署\n",
    "def policy_iteration(env, tol=1e-20):\n",
    "\n",
    "    vs = []\n",
    "    v  = np.zeros(env.n_states)\n",
    "    q  = np.zeros((env.n_states, env.n_actions))\n",
    "    pi = np.zeros(env.n_states, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        v_old = np.copy(v)\n",
    "        v, q = evaluate_policy(pi, env, tol)  # 第一步 π_(t)  →  V^π Q^π\n",
    "        pi   = get_greedy_policy(q)           # 第二部 Q^π    →  π_(t+1)\n",
    "        vs.append(v)\n",
    "\n",
    "        # 终止条件\n",
    "        if np.linalg.norm(v - v_old) < tol:\n",
    "            break\n",
    "\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题：\n",
    "\n",
    "- 固定策略下，贝尔曼 策略算子 $T^\\pi$ 和 最优算子 $T^*$ 的区别\n",
    "- 随机策略下，贝尔曼 策略算子 $T^\\pi$ 和 最优算子 $T^*$ 的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, tol=1e-10):\n",
    "    # 初始化\n",
    "    v = np.zeros(env.n_states)                    # initialize value function\n",
    "    q = np.zeros((env.n_states, env.n_actions))   # initialize Q-value function\n",
    "    \n",
    "    # 迭代\n",
    "    while True:\n",
    "        v_old = np.copy(v)  \n",
    "    \n",
    "        # 贝尔曼-最优算子\n",
    "        for a in range(env.n_actions):\n",
    "            q[:, a] = env.r[:,a] + env.gamma * env.sparseT[a].dot(v)  \n",
    "            # q = 9x4 矩阵 (代表每个(s,a)组合下的 回报期望 = 即刻回报 + 下一步折扣期望回报)\n",
    "\n",
    "        v = np.max(q,axis = 1)  \n",
    "        # v = 9x1 向量 (选出最优的a使得回报最大 = 选出每行最大值)\n",
    "\n",
    "        # 终止标准\n",
    "        if np.linalg.norm(v - v_old) < tol: \n",
    "            v_star = v\n",
    "            break\n",
    "            \n",
    "    return v_star\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
